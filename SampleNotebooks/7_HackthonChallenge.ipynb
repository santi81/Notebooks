{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML bootcamp hackthon challenge:  predict the outcome of EURO 2016 soccer matches for fun and profit\n",
    "\n",
    "\n",
    "At this stage you have learned what machine learning is and you have seen examples of how to use machine learning. Now it is time to see how well YOU can generalize what you have learned to a new challenge.\n",
    "\n",
    "We will be using machine learning to **predict the outcome of EURO 2016 soccer matches for fun and profit**.\n",
    "The hackathon is based on the code of the following repo:\n",
    "https://github.wdf.sap.corp/I073941/euro2016_ml\n",
    "\n",
    "The code below will help you to get started. At the final pitch session, teams can present their models to the team and to test their system on how accurately they would have predicted the games in the EURO 2016 competition.\n",
    "\n",
    "Check out the report from Goldman Sachs about predicting the 2014 worldcup.\n",
    "http://www.goldmansachs.com/our-thinking/outlook/world-cup-sections/world-cup-book-2014-statistical-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages that we need\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from subprocess import call\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "\n",
    "# set random seed, make results reproducible\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper functions to download and parse historic soccer matches\n",
    "# from http://www.eloratings.net/\n",
    "#\n",
    "# Note: Elo rating system is a method for calculating the relative skill levels of players in \n",
    "# games such as chesshttps://en.wikipedia.org/wiki/Elo_rating_system)\n",
    "\n",
    "def load_elo(country):\n",
    "    \"\"\"download ELO ratings the web, and parse html pages, return data frame\"\"\"\n",
    "    print(\"load %s..\" % country)\n",
    "    url_template = \"http://www.eloratings.net/%s.htm\"\n",
    "    data_dir = '/tmp/elo/'\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    elo_file = \"%s/ELO_%s.htm\" % (data_dir, country)\n",
    "    # NOTE: urllib gets a HTTP 412 error, use curl workaround for download\n",
    "    if not os.path.isfile(elo_file):\n",
    "        call ([\"curl\", url_template % country, \"-o\", elo_file, \"-x\", \"http://proxy.wdf.sap.corp:8080\"])\n",
    "    with open(elo_file) as fin:\n",
    "        html = fin.read()\n",
    "    historic_games = list(parse_elo(html))\n",
    "    # at the moment we only extract team names, the match score, match type, date and the ELO score of each team\n",
    "    # there are more columns, consider adding them as additional attributes\n",
    "    df = pd.DataFrame(historic_games, columns=[\"Team1\", \"Team2\", \"Score1\", \"Score2\", \"ELO1\", \"ELO2\", \"MatchType\", \"Date\"])\n",
    "    return df\n",
    "\n",
    "def text_with_newline(elem):\n",
    "    text = ''\n",
    "    for e in elem.recursiveChildGenerator():\n",
    "        if isinstance(e, str):\n",
    "            text += e.strip()\n",
    "        elif e.name == 'br':\n",
    "            text += '\\n'\n",
    "    return text\n",
    "\n",
    "def clean_string(s):\n",
    "    # remove non-breaking space character\n",
    "    s = s.replace(u'\\xa0', u' ')\n",
    "    return s\n",
    "\n",
    "def parse_elo(html):\n",
    "    \"\"\"Parse ELO ratings and extract dates and ranking\"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # find the main result table\n",
    "    table = soup.body.find('table', class_='results')\n",
    "    for row in table.find_all('tr', class_='nh'):\n",
    "        fields = row.find_all('td')\n",
    "        if len(fields) < 6:\n",
    "            continue\n",
    "        date = parse(text_with_newline(fields[0]).replace('\\n', ' '))\n",
    "        team1, team2 = clean_string(text_with_newline(fields[1])).split('\\n')\n",
    "        score1, score2 = map(lambda x: int(x), text_with_newline(fields[2]).split('\\n'))\n",
    "        elo1, elo2 = map(lambda x: int(x), text_with_newline(fields[5]).split('\\n'))\n",
    "        match_type = clean_string(text_with_newline(fields[3]).replace('\\n', ' ')).strip()\n",
    "        # the elo rating is already adjusted to the outcome of the game\n",
    "        # so calculate what the rating has been before the game\n",
    "        elo1_change, elo2_change = map(lambda x: int(x), text_with_newline(fields[4]).split('\\n'))\n",
    "        elo1 -= elo1_change\n",
    "        elo2 -= elo2_change\n",
    "        yield team1, team2, score1, score2, elo1, elo2, match_type, date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load Albania..\n",
      "load Austria..\n",
      "load Belgium..\n",
      "load Croatia..\n",
      "load Czechia..\n",
      "load England..\n",
      "load France..\n",
      "load Germany..\n",
      "load Hungary..\n",
      "load Iceland..\n",
      "load Italy..\n",
      "load Nthrn_Irelnd..\n",
      "load Poland..\n",
      "load Portugal..\n",
      "load Ireland..\n",
      "load Romania..\n",
      "load Russia..\n",
      "load Slovakia..\n",
      "load Spain..\n",
      "load Sweden..\n",
      "load Switzerland..\n",
      "load Turkey..\n",
      "load Ukraine..\n",
      "load Wales..\n",
      "=== training data ===\n",
      "         Team1     Team2  Score1  Score2  ELO1  ELO2             MatchType  \\\n",
      "2838  Scotland   England       0       0  1800  1800  Friendly in Scotland   \n",
      "2839   England  Scotland       4       2  1803  1797   Friendly in England   \n",
      "2840  Scotland   England       2       1  1786  1814  Friendly in Scotland   \n",
      "2841   England  Scotland       2       2  1806  1794   Friendly in England   \n",
      "2842  Scotland   England       3       0  1797  1803  Friendly in Scotland   \n",
      "\n",
      "           Date  \n",
      "2838 1872-11-30  \n",
      "2839 1873-03-08  \n",
      "2840 1874-03-07  \n",
      "2841 1875-03-06  \n",
      "2842 1876-03-04  \n",
      "\n",
      "\n",
      "=== EURO2016 ===\n",
      "             Team1     Team2  Score1  Score2  ELO1  ELO2  \\\n",
      "4592        France   Romania       2       1  1952  1729   \n",
      "10806       France   Romania       2       1  1952  1729   \n",
      "3800        Russia   England       1       1  1736  1947   \n",
      "303    Switzerland   Albania       1       0  1745  1584   \n",
      "15640        Wales  Slovakia       2       1  1629  1742   \n",
      "\n",
      "                             MatchType       Date  \n",
      "4592   European Championship in France 2016-06-10  \n",
      "10806  European Championship in France 2016-06-10  \n",
      "3800   European Championship in France 2016-06-11  \n",
      "303    European Championship in France 2016-06-11  \n",
      "15640  European Championship in France 2016-06-11  \n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "countries = [\"Albania\", \"Austria\", \"Belgium\", \"Croatia\", \"Czech Republic\", \"England\", \"France\", \"Germany\", \"Hungary\", \"Iceland\", \"Italy\", \"Northern Ireland\", \"Poland\", \"Portugal\", \"Ireland\", \"Romania\", \"Russia\", \"Slovakia\", \"Spain\", \"Sweden\", \"Switzerland\", \"Turkey\", \"Ukraine\", \"Wales\"]\n",
    "country_canonical = {\"Czech Republic\": \"Czechia\", \"Northern Ireland\": \"Nthrn_Irelnd\"}\n",
    "df = pd.DataFrame([], columns=[\"Team1\", \"Team2\", \"Score1\", \"Score2\", \"ELO1\", \"ELO2\", \"Date\"])\n",
    "\n",
    "# load elo ranking\n",
    "df = pd.concat((load_elo(country_canonical.get(country, country)) for country in countries), ignore_index=True).sort_values(\"Date\")\n",
    "\n",
    "# split data frame into EURO 2016 games (test) and others (train)\n",
    "euro2016_start= datetime.date(2016, 6, 10)\n",
    "euro2016_end= datetime.date(2016, 7, 10)\n",
    "\n",
    "is_euro2016 = (df['MatchType'] == \"European Championship in France\") & (df['Date'] >= euro2016_start) & (df['Date'] <= euro2016_end)\n",
    "df_euro2016 = df.loc[ is_euro2016 ]\n",
    "df_train = df.loc[ ~is_euro2016 ]\n",
    "\n",
    "# print the first 5 rows of each data frame and look at the data\n",
    "# you can do more advanced data exploration and plotting here\n",
    "print(\"=== training data ===\")\n",
    "print(df_train.head(5))\n",
    "print(\"\\n\\n=== EURO2016 ===\")\n",
    "print(df_euro2016.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_learning_to_rank(df):\n",
    "    \"\"\"Create learning to rank feature matrix X and labels y. Yield (x, y) instances as generator.\"\"\"\n",
    "    for row in df.iterrows():\n",
    "        score_delta = row[1].Score1 - row[1].Score2\n",
    "        elo_delta = row[1].ELO1 - row[1].ELO2\n",
    "        # learning to rank as binary classification                                                                                             \n",
    "        if score_delta == 0:\n",
    "            # draw, ignore for learning to rank                                                                         \n",
    "            continue\n",
    "        label = np.sign(score_delta)\n",
    "        # at the moment the ELO delta is the only feature, you can add more features here\n",
    "        features = [ elo_delta ]\n",
    "        # yield (x, y)                                                                                                  \n",
    "        yield (features, label)\n",
    "        # yield symmetric instance (-x, -y)                                                                             \n",
    "        yield (list(map(lambda x: -x, features)), -label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Learning to rank ==\n",
      "Baseline random guess\n",
      "Accuracy random : 0.4953 \n",
      "\n",
      "Cross-validation for model selection\n",
      "logistic regression SGD\n",
      "Accuracy: 0.68 (+/- 0.27)\n",
      "--> New best score\n",
      "\n",
      "perceptron SGD\n",
      "Accuracy: 0.69 (+/- 0.25)\n",
      "--> New best score\n",
      "\n",
      "SGD hinge\n",
      "Accuracy: 0.64 (+/- 0.34)\n",
      "\n",
      "Passive Agressive\n",
      "Accuracy: 0.50 (+/- 0.42)\n",
      "\n",
      "Test accuracy on EURO 2016 with model perceptron SGD : 0.56\n"
     ]
    }
   ],
   "source": [
    "## learning to rank                                                                                                 \n",
    "# generate features and labels                                                                                      \n",
    "print(\"== Learning to rank ==\")\n",
    "Xy_train = list(generate_learning_to_rank(df_train))\n",
    "X_train = np.array([x for x,y in Xy_train])\n",
    "y_train = np.array([y for x,y in Xy_train])\n",
    "\n",
    "# random guess                                                                                                      \n",
    "y_rand = [random.choice([-1, 1]) for _ in range(len(y_train))]\n",
    "print(\"Baseline random guess\")\n",
    "print(\"Accuracy random : %.4f \" % accuracy_score(y_train, y_rand))\n",
    "print(\"\")\n",
    "\n",
    "# cross validation with different classifiers\n",
    "models = [(\"logistic regression SGD\", SGDClassifier(loss=\"log\")), (\"perceptron SGD\", SGDClassifier(loss\\\n",
    "=\"perceptron\")), (\"SGD hinge\", SGDClassifier(loss=\"hinge\")), (\"Passive Agressive\", PassiveAggressiveClassifier())]\n",
    "best_model = None\n",
    "best_score = 0.0\n",
    "print(\"Cross-validation for model selection\")\n",
    "for clf_name, clf in models:\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=10)\n",
    "    print(clf_name)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    if best_model == None or scores.mean() > best_score:\n",
    "        print(\"--> New best score\")\n",
    "        best_score = scores.mean()\n",
    "        best_model = (clf_name, clf)\n",
    "    print(\"\")\n",
    "\n",
    "# train best model on the whote training data and test on the EURO 2016\n",
    "clf_name, clf = best_model\n",
    "clf.fit(X_train, y_train)\n",
    "Xy_test = list(generate_learning_to_rank(df_euro2016))\n",
    "X_test = np.array([x for x,y in Xy_test])\n",
    "y_test = np.array([y for x,y in Xy_test])\n",
    "y_guess = clf.predict(X_test)\n",
    "print(\"Test accuracy on EURO 2016 with model %s : %0.2f\" % (clf_name, float(sum(y_test == y_guess)) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What to do next\n",
    "Congratulations, you have created a simple learning-to-rank binary classifier! \n",
    "\n",
    "You can try to improve this classifiers by **adding more features**, for example a feature which team is the home team, whether it is a friendly match or a competitive match, you can try to give more weight to recent training examples or create trend features like 'percentage of matches won out of the last 5 matches' (be careful with time-dependent features when you do cross-validation, you need to re-compute features depending on the test instance).\n",
    "\n",
    "Or you can **try out more machine learning classifiers** and try to **fine-tune the hyper-parameters** of each model.\n",
    "\n",
    "You can also extend this notebook to **more advanced tasks**: try making this a 3-way multi-class classification where you predict win, lose, draw, or if that is still to easy, try to convert it to a regression problem and predict the exact score of the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
