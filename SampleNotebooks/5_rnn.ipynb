{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "In the following we will implement and train an RNN for text classification. The following example will use the ATIS database. The ATIS database consists of data obtained from the Official Airline Guide (OAG, 1990), organized under a relational schema. The database remained fixed throughout the pilot phase. It contains information about flights, fares, airlines, cities, airports, and ground services, and includes twenty-five supporting tables. The large majority of the questions posed by subjects can be answered from the database with a single relational query.\n",
    "\n",
    "For more information about RNN check out the following resources:\n",
    "\n",
    "[RNN with Theano](http://deeplearning.net/tutorial/rnnslu.html)\n",
    "\n",
    "[Nice blog about RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "[Amazing Stanford lecture about NLP (natural language processing)][http://cs224d.stanford.edu/]\n",
    "\n",
    "For advanced text processing:\n",
    "\n",
    "[Long-Short-Term-Memory (LSTM) a RNN variant](http://deeplearning.net/tutorial/lstm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano, numpy\n",
    "from theano import tensor as T\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import sys\n",
    "import gzip\n",
    "import urllib\n",
    "import random\n",
    "import stat\n",
    "import subprocess\n",
    "import sys\n",
    "import timeit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "sys.setrecursionlimit(1500)\n",
    "\n",
    "# get home directory\n",
    "from os.path import expanduser\n",
    "HOME_PREFIX = expanduser('~')\n",
    "SCRIPT_PREFIX = \"/scripts\"\n",
    "DATA_PREFIX = \"/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data loading functions\n",
    "def atisfold(fold):\n",
    "    assert fold in range(5)\n",
    "    filename = os.path.join(DATA_PREFIX, 'atis/atis.fold'+str(fold)+'.pkl.gz')\n",
    "    print(filename)\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f,encoding='latin1')\n",
    "    return train_set, valid_set, test_set, dicts\n",
    "\n",
    "def shuffle(lol, seed):\n",
    "    '''\n",
    "    lol :: list of list as input\n",
    "    seed :: seed the shuffling\n",
    "\n",
    "    shuffle inplace each list in the same order\n",
    "    '''\n",
    "    for l in lol:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(l)\n",
    "\n",
    "def contextwin(l, win):\n",
    "    '''\n",
    "    win :: int corresponding to the size of the window\n",
    "    given a list of indexes composing a sentence\n",
    "\n",
    "    l :: array containing the word indexes\n",
    "\n",
    "    it will return a list of list of indexes corresponding\n",
    "    to context windows surrounding each word in the sentence\n",
    "    '''\n",
    "    assert (win % 2) == 1\n",
    "    assert win >= 1\n",
    "    l = list(l)\n",
    "\n",
    "    lpadded = win // 2 * [-1] + l + win // 2 * [-1]\n",
    "    out = [lpadded[i:(i + win)] for i in range(len(l))]\n",
    "\n",
    "    assert len(out) == len(l)\n",
    "    return out\n",
    "\n",
    "def conlleval(p, g, w, filename):\n",
    "    '''\n",
    "    INPUT:\n",
    "    p :: predictions\n",
    "    g :: groundtruth\n",
    "    w :: corresponding words\n",
    "    OUTPUT:\n",
    "    filename :: name of the file where the predictions\n",
    "    are written. it will be the input of conlleval.pl script\n",
    "    for computing the performance in terms of precision\n",
    "    recall and f1 score\n",
    "    '''\n",
    "    out = ''\n",
    "    for sl, sp, sw in zip(g, p, w):\n",
    "        out += 'BOS O O\\n'\n",
    "        for wl, wp, w in zip(sl, sp, sw):\n",
    "            out += w + ' ' + wl + ' ' + wp + '\\n'\n",
    "        out += 'EOS O O\\n\\n'\n",
    "\n",
    "    f = open(filename,'w')\n",
    "    f.writelines(out)\n",
    "    f.close()\n",
    "    \n",
    "    return get_perf(filename)\n",
    "\n",
    "def get_perf(filename):\n",
    "    ''' run conlleval.pl perl script to obtain\n",
    "    precision/recall and F1 score '''\n",
    "    _conlleval = os.path.join(SCRIPT_PREFIX, 'conlleval.pl')\n",
    "\n",
    "    proc = subprocess.Popen([\"perl\", _conlleval], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    stdout, _ = proc.communicate(open(filename).read().encode('utf-8'))\n",
    "\n",
    "    for line in stdout.decode().split('\\n'):\n",
    "        if 'accuracy' in line:\n",
    "            out = line.split()\n",
    "            break\n",
    "    \n",
    "    # out = ['accuracy:', '16.26%;', 'precision:', '0.00%;', 'recall:', '0.00%;', 'FB1:', '0.00']\n",
    "    \n",
    "    precision = float(out[3][:-2])\n",
    "    recall    = float(out[5][:-2])\n",
    "    f1score   = float(out[7])\n",
    "\n",
    "    return {'p':precision, 'r':recall, 'f1':f1score}\n",
    "\n",
    "def arraysentence2string(arr):\n",
    "    return \" \".join([idx2word[x] for x in arr if x > 0])\n",
    "\n",
    "def arraylabels2string(arr):\n",
    "    return \" \".join([idx2label[x] for x in arr if x > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_set, valid_set, test_set, dic = atisfold(0)\n",
    "import six\n",
    "\n",
    "idx2label = dict((k, v) for v, k in six.iteritems(dic['labels2idx']))\n",
    "idx2word = dict((k, v) for v, k in six.iteritems(dic['words2idx']))\n",
    "\n",
    "train_lex, train_ne, train_y = train_set\n",
    "valid_lex, valid_ne, valid_y = valid_set\n",
    "test_lex, test_ne, test_y = test_set\n",
    "\n",
    "vocsize = len(dic['words2idx'])\n",
    "print (\"Dictionary size: \"+str(vocsize))\n",
    "nclasses = len(dic['labels2idx'])\n",
    "print (\"Number of class labels: \"+str(nclasses))\n",
    "nsentences = len(train_lex)\n",
    "\n",
    "groundtruth_valid = list(list(map(lambda x: idx2label[x], y)) for y in valid_y)\n",
    "words_valid = list(list(map(lambda x: idx2word[x], w)) for w in valid_lex)\n",
    "groundtruth_test = list(list(map(lambda x: idx2label[x], y)) for y in test_y)\n",
    "words_test = list(list(map(lambda x: idx2word[x], w)) for w in test_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (train_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the data look like?\n",
    "In order to represent sentences and to make them machine understandable, data has to be processed. At first a dictionary is built. Within the dictionary each word is associated with an index. Given the dictionary and a sentence, a normal textural sentence is converted an array of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first sentence in the training data set\n",
    "train_lex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first sentence in the training data set\n",
    "example_sentence = train_lex[0]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(arraysentence2string(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The first label in the training data set\n",
    "example_labels = train_y[0]\n",
    "print(example_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(arraylabels2string(example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context window\n",
    "Words in itself can be ambigious, therefore it is useful to consider them in their context. As an example the word 'Paris' could relate to the actress 'Paris Hilton' or the city of Paris. Therefore each word is normally considered within its context. That is we look what comes before and after the word we are interested. Special cases are the beginning and the end of a sentence. In that case we use padding (-1) to highlight this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "win_size = 5 \n",
    "csample = contextwin(example_sentence, win_size)\n",
    "\n",
    "for x in csample:\n",
    "        print(x)\n",
    "        print(arraysentence2string(x)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's define a RNN\n",
    "class RNNSLU(object):\n",
    "    ''' elman neural net model '''\n",
    "    def __init__(self, nh, nc, ne, de, cs):\n",
    "        '''\n",
    "        nh :: dimension of the hidden layer\n",
    "        nc :: number of classes\n",
    "        ne :: number of word embeddings in the vocabulary / size of vocabulary\n",
    "        de :: dimension of the word embeddings\n",
    "        cs :: word window context size\n",
    "        '''\n",
    "        \n",
    "        # parameters of the model\n",
    "        self.emb = theano.shared(name='embeddings',\n",
    "                                 value=0.2 * numpy.random.uniform(-1.0, 1.0,\n",
    "                                 (ne+1, de))\n",
    "                                 # add one for padding at the end\n",
    "                                 .astype(theano.config.floatX))\n",
    "        self.wx = theano.shared(name='wx',\n",
    "                                value=0.2 * numpy.random.uniform(-1.0, 1.0,\n",
    "                                (de * cs, nh))\n",
    "                                .astype(theano.config.floatX))\n",
    "        self.wh = theano.shared(name='wh',\n",
    "                                value=0.2 * numpy.random.uniform(-1.0, 1.0,\n",
    "                                (nh, nh))\n",
    "                                .astype(theano.config.floatX))\n",
    "        self.w = theano.shared(name='w',\n",
    "                               value=0.2 * numpy.random.uniform(-1.0, 1.0,\n",
    "                               (nh, nc))\n",
    "                               .astype(theano.config.floatX))\n",
    "        self.bh = theano.shared(name='bh',\n",
    "                                value=numpy.zeros(nh,\n",
    "                                dtype=theano.config.floatX))\n",
    "        self.b = theano.shared(name='b',\n",
    "                               value=numpy.zeros(nc,\n",
    "                               dtype=theano.config.floatX))\n",
    "        self.h0 = theano.shared(name='h0',\n",
    "                                value=numpy.zeros(nh,\n",
    "                                dtype=theano.config.floatX))\n",
    "        \n",
    "        \n",
    "\n",
    "        # bundle the parameters together, nice for gradient computation\n",
    "        self.params = [self.emb, self.wx, self.wh, self.w,\n",
    "                       self.bh, self.b, self.h0]\n",
    "        \n",
    "        # we perform training on a complete sentence, which is broken down into context window elements\n",
    "        \n",
    "        # input is a matrix full of indices:\n",
    "        # as many columns as context window size\n",
    "        # as many lines as words in the sentence (each word is presented within context window)\n",
    "        idxs = T.imatrix()\n",
    "        \n",
    "        # map the input to the embedding space\n",
    "        x = self.emb[idxs].reshape((idxs.shape[0], de*cs))\n",
    "        \n",
    "        # labels for the words in the stenence\n",
    "        y_sentence = T.ivector('y_sentence')\n",
    "        \n",
    "        \n",
    "        # basic Elmann recurrent networks\n",
    "        \n",
    "        def recurrence(x_t, h_tm1):\n",
    "            \n",
    "            # hidden state\n",
    "            h_t = T.nnet.sigmoid(T.dot(x_t, self.wx)\n",
    "                                 + T.dot(h_tm1, self.wh) + self.bh)\n",
    "            \n",
    "            # output\n",
    "            s_t = T.nnet.softmax(T.dot(h_t, self.w) + self.b)\n",
    "            \n",
    "            return [h_t, s_t]\n",
    "\n",
    "        # looping over the words in the sentence\n",
    "        [h, s], _ = theano.scan(fn=recurrence, #first argument is the time sliced input\n",
    "                                # input\n",
    "                                sequences=x,\n",
    "                                # initial value, must have the same dimension as output [h_t, s_t]\n",
    "                                outputs_info=[self.h0, None],\n",
    "                                # number of steps in the loop (corresponds to words in the sentence)\n",
    "                                n_steps=x.shape[0])\n",
    "\n",
    "        # output of the RNN is softmax result (probabilities)\n",
    "        p_y_given_x_sentence = s[:, 0, :]\n",
    "        \n",
    "        # for each word return the class with maximum probability\n",
    "        y_pred = T.argmax(p_y_given_x_sentence, axis=1)\n",
    "        \n",
    "\n",
    "        # cost and gradients and learning rate\n",
    "        lr = T.scalar('lr')\n",
    "\n",
    "        # cost function is the negative log-likelihood (NLL)\n",
    "        \n",
    "        \n",
    "        # x.shape[0] is (symbolically) the number of rows in x, i.e.,\n",
    "        # number of words in the sentence (call it n) in the minibatch\n",
    "        # T.arange(x.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x_sentence) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per word and\n",
    "        # one column per class LP[T.arange(x.shape[0]),y_sentence] is a vector\n",
    "        # v containing [LP[0,y_sentence[0]], LP[1,y_sentence[1]], LP[2,y_sentence[2]], ...,\n",
    "        # LP[n-1,y_sentence[n-1]]] and T.mean(LP[T.arange(x.shape[0]),y_sentence]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        \n",
    "        sentence_nll = -T.mean(T.log(p_y_given_x_sentence)\n",
    "                               [T.arange(x.shape[0]), y_sentence])\n",
    "        \n",
    "        # now let theano do the ugly work and compute the SYMBOLIC gradients\n",
    "        sentence_gradients = T.grad(sentence_nll, self.params)\n",
    "        \n",
    "        # the updates is an ordered dicitionary, where each pair is a parameter and how it is updated (gradient descent)\n",
    "        sentence_updates = OrderedDict((p, p - lr*g)\n",
    "                                       for p, g in\n",
    "                                       zip(self.params, sentence_gradients))\n",
    "    \n",
    "\n",
    "        # theano functions to compile\n",
    "        \n",
    "        # classification function: \n",
    "        # INPUT: sentence (put in context windows)\n",
    "        # RETURN: vector (class for each word)\n",
    "        self.classify = theano.function(inputs=[idxs], outputs=y_pred)\n",
    "        \n",
    "        # training function:\n",
    "        # INPUT: sentence (put in context windows), class labels for each word (vector), and a learning rate\n",
    "        # RETURN: cost (negative log likelihood)\n",
    "        self.sentence_train = theano.function(inputs=[idxs, y_sentence, lr],\n",
    "                                              outputs=sentence_nll, \n",
    "                                              updates=sentence_updates)\n",
    "        \n",
    "        # normalization function (normalization of the embedding)\n",
    "        # update the embedding, such that it stays on the unit-ball (L2 norm equal to 1)\n",
    "        # dimshuffle(0, 'x') ->  make a column out of a 1d vector (N to Nx1)\n",
    "        self.normalize = theano.function(inputs=[],\n",
    "                                         updates={self.emb:\n",
    "                                                  self.emb /\n",
    "                                                  T.sqrt((self.emb**2)\n",
    "                                                  .sum(axis=1))\n",
    "                                                  .dimshuffle(0, 'x')})\n",
    "\n",
    "\n",
    "    def train(self, x, y, window_size, learning_rate):\n",
    "\n",
    "        cwords = contextwin(x, window_size)\n",
    "        #words = map(lambda x: numpy.asarray(x).astype('int32'), cwords)\n",
    "        words = cwords\n",
    "        labels = y\n",
    "        \n",
    "        #print(words,flush=True)\n",
    "\n",
    "        self.sentence_train(words, labels, learning_rate)\n",
    "        self.normalize()\n",
    "\n",
    "    def save(self, folder):\n",
    "        for param in self.params:\n",
    "            numpy.save(os.path.join(folder,\n",
    "                       param.name + '.npy'), param.get_value())\n",
    "\n",
    "    def load(self, folder):\n",
    "        for param in self.params:\n",
    "            param.set_value(numpy.load(os.path.join(folder,\n",
    "                            param.name + '.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN (training) parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    'fold': 3,\n",
    "    # 5 folds 0,1,2,3,4\n",
    "    'data': 'atis',\n",
    "    'lr': 0.0970806646812754,\n",
    "    'verbose': 1,\n",
    "    'decay': True,\n",
    "    # decay on the learning rate if improvement stops\n",
    "    'win': 5,\n",
    "    # number of words in the context window\n",
    "    'nhidden': 50,\n",
    "    # number of hidden units\n",
    "    'seed': 345,\n",
    "    'emb_dimension': 3,\n",
    "    # dimension of word embedding\n",
    "    'nepochs': 30,\n",
    "    # 60 is recommended\n",
    "    'savemodel': False}\n",
    "print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = \"RNN\"\n",
    "folder = os.path.join(HOME_PREFIX, folder_name)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "\n",
    "\n",
    "# instanciate the model\n",
    "numpy.random.seed(param['seed'])\n",
    "random.seed(param['seed'])\n",
    "\n",
    "rnn = RNNSLU(nh=param['nhidden'],\n",
    "             nc=nclasses,\n",
    "             ne=vocsize,\n",
    "             de=param['emb_dimension'],\n",
    "             cs=param['win'])\n",
    "\n",
    "# train with early stopping on validation set\n",
    "best_f1 = -numpy.inf\n",
    "param['clr'] = param['lr']\n",
    "for e in range(param['nepochs']):\n",
    "\n",
    "    # shuffle\n",
    "    shuffle([train_lex, train_ne, train_y], param['seed'])\n",
    "\n",
    "    param['ce'] = e\n",
    "    tic = timeit.default_timer()\n",
    "\n",
    "    # for each sentence in the training compute gradient and update\n",
    "    for i, (x, y) in enumerate(zip(train_lex, train_y)):\n",
    "        rnn.train(x, y, param['win'], param['clr'])\n",
    "        #print ('[learning] epoch '+str(e)+' >> '+str(float(\"{0:.2f}\".format((i+1)* 100. / nsentences))))\n",
    "        #print ('completed in %.2f (sec) <<\\r' % (timeit.default_timer() - tic), file=sys.stdout, flush=True)\n",
    "       \n",
    "\n",
    "    # evaluation // back into the real world : idx -> words\n",
    "    predictions_test = [map(lambda x: idx2label[x],\n",
    "                        rnn.classify(numpy.asarray(\n",
    "                        contextwin(x, param['win'])).astype('int32')))\n",
    "                        for x in test_lex]\n",
    "    predictions_valid = [map(lambda x: idx2label[x],\n",
    "                         rnn.classify(numpy.asarray(\n",
    "                         contextwin(x, param['win'])).astype('int32')))\n",
    "                         for x in valid_lex]\n",
    "\n",
    "    # evaluation // compute the accuracy using conlleval.pl\n",
    "    res_test = conlleval(predictions_test,\n",
    "                         groundtruth_test,\n",
    "                         words_test,\n",
    "                         folder + '/current.test.txt')\n",
    "    res_valid = conlleval(predictions_valid,\n",
    "                          groundtruth_valid,\n",
    "                          words_valid,\n",
    "                          folder + '/current.valid.txt')\n",
    "\n",
    "    if res_valid['f1'] > best_f1:\n",
    "\n",
    "        if param['savemodel']:\n",
    "            rnn.save(folder)\n",
    "\n",
    "        best_rnn = copy.deepcopy(rnn)\n",
    "        best_f1 = res_valid['f1']\n",
    "\n",
    "        if param['verbose']:\n",
    "            print('NEW BEST: epoch', e,\n",
    "                  'valid F1', res_valid['f1'],\n",
    "                  'best test F1', res_test['f1'], flush=True)\n",
    "\n",
    "        param['vf1'], param['tf1'] = res_valid['f1'], res_test['f1']\n",
    "        param['vp'], param['tp'] = res_valid['p'], res_test['p']\n",
    "        param['vr'], param['tr'] = res_valid['r'], res_test['r']\n",
    "        param['be'] = e\n",
    "\n",
    "        subprocess.call(['mv', folder + '/current.test.txt',\n",
    "                        folder + '/best.test.txt'])\n",
    "        subprocess.call(['mv', folder + '/current.valid.txt',\n",
    "                        folder + '/best.valid.txt'])\n",
    "    else:\n",
    "        if param['verbose']:\n",
    "            print('')\n",
    "\n",
    "    # learning rate decay if no improvement in 10 epochs\n",
    "    if param['decay'] and abs(param['be']-param['ce']) >= 10:\n",
    "        param['clr'] *= 0.5\n",
    "        rnn = best_rnn\n",
    "\n",
    "    if param['clr'] < 1e-5:\n",
    "        break\n",
    "\n",
    "print('BEST RESULT: epoch', param['be'],\n",
    "      'valid F1', param['vf1'],\n",
    "      'best test F1', param['tf1'],\n",
    "      'with the model', folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we have a look at the embedding of the words in the vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rnn.emb.eval().shape)\n",
    "# make sure the row-wise norm is equal to 1\n",
    "assert(np.sum(np.apply_along_axis(np.linalg.norm, 1, rnn.emb.eval())) == rnn.emb.eval().shape[0]),\"Embedding not properly normalized: Each row has to have norm 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we compute check the nearest neighbors for all elements for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(rnn.emb.eval())\n",
    "distances, indices = nbrs.kneighbors(rnn.emb.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the different label categories?\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the nearest neighbor to the word 'philadelphia'\n",
    "idx = 376\n",
    "[idx2word[x] for x in indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the nearest neighbor to the word 'saturday'\n",
    "idx = 420\n",
    "[idx2word[x] for x in indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the nearest neighbor to the word 'august'\n",
    "idx = 65\n",
    "[idx2word[x] for x in indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show the nearest neighbor to the word 'breakfast'\n",
    "idx = 80\n",
    "[idx2word[x] for x in indices[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
